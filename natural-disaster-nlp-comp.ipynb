{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# ToDo:\n- preprocess text - lemmatization, remove stop words etc. \n- utlise preprocessed text\n- Try DistilBert https://www.kaggle.com/code/alexia/kerasnlp-starter-notebook-disaster-tweets\n- sklearn pipelines\n- cross validation\n","metadata":{}},{"cell_type":"code","source":"!pip install nlpaug sentencepiece sacremoses","metadata":{"execution":{"iopub.status.busy":"2023-08-07T15:17:43.870159Z","iopub.execute_input":"2023-08-07T15:17:43.870694Z","iopub.status.idle":"2023-08-07T15:18:03.181412Z","shell.execute_reply.started":"2023-08-07T15:17:43.870657Z","shell.execute_reply":"2023-08-07T15:18:03.180121Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Collecting nlpaug\n  Downloading nlpaug-1.1.11-py3-none-any.whl (410 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.5/410.5 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hRequirement already satisfied: sentencepiece in /opt/conda/lib/python3.10/site-packages (0.1.99)\nCollecting sacremoses\n  Downloading sacremoses-0.0.53.tar.gz (880 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m880.6/880.6 kB\u001b[0m \u001b[31m29.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: numpy>=1.16.2 in /opt/conda/lib/python3.10/site-packages (from nlpaug) (1.23.5)\nRequirement already satisfied: pandas>=1.2.0 in /opt/conda/lib/python3.10/site-packages (from nlpaug) (1.5.3)\nRequirement already satisfied: requests>=2.22.0 in /opt/conda/lib/python3.10/site-packages (from nlpaug) (2.31.0)\nCollecting gdown>=4.0.0 (from nlpaug)\n  Downloading gdown-4.7.1-py3-none-any.whl (15 kB)\nRequirement already satisfied: regex in /opt/conda/lib/python3.10/site-packages (from sacremoses) (2023.6.3)\nRequirement already satisfied: six in /opt/conda/lib/python3.10/site-packages (from sacremoses) (1.16.0)\nRequirement already satisfied: click in /opt/conda/lib/python3.10/site-packages (from sacremoses) (8.1.3)\nRequirement already satisfied: joblib in /opt/conda/lib/python3.10/site-packages (from sacremoses) (1.2.0)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from sacremoses) (4.65.0)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from gdown>=4.0.0->nlpaug) (3.12.2)\nRequirement already satisfied: beautifulsoup4 in /opt/conda/lib/python3.10/site-packages (from gdown>=4.0.0->nlpaug) (4.12.2)\nRequirement already satisfied: python-dateutil>=2.8.1 in /opt/conda/lib/python3.10/site-packages (from pandas>=1.2.0->nlpaug) (2.8.2)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas>=1.2.0->nlpaug) (2023.3)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests>=2.22.0->nlpaug) (3.1.0)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests>=2.22.0->nlpaug) (3.4)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests>=2.22.0->nlpaug) (1.26.15)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests>=2.22.0->nlpaug) (2023.5.7)\nRequirement already satisfied: soupsieve>1.2 in /opt/conda/lib/python3.10/site-packages (from beautifulsoup4->gdown>=4.0.0->nlpaug) (2.3.2.post1)\nRequirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /opt/conda/lib/python3.10/site-packages (from requests>=2.22.0->nlpaug) (1.7.1)\nBuilding wheels for collected packages: sacremoses\n  Building wheel for sacremoses (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for sacremoses: filename=sacremoses-0.0.53-py3-none-any.whl size=895260 sha256=f18dc0345ffe99013e813fb355c011b681d53860b9dc36586f0e63269e3db797\n  Stored in directory: /root/.cache/pip/wheels/00/24/97/a2ea5324f36bc626e1ea0267f33db6aa80d157ee977e9e42fb\nSuccessfully built sacremoses\nInstalling collected packages: sacremoses, gdown, nlpaug\nSuccessfully installed gdown-4.7.1 nlpaug-1.1.11 sacremoses-0.0.53\n","output_type":"stream"}]},{"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nfrom sklearn import feature_extraction, linear_model, model_selection, preprocessing\nfrom sklearn.model_selection import train_test_split, cross_validate\nfrom sklearn.metrics import f1_score, r2_score, accuracy_score\nfrom sklearn.gaussian_process import GaussianProcessClassifier\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.pipeline import Pipeline\nimport xgboost\nimport spacy\nfrom transformers import AutoTokenizer\nimport torch\nimport datasets\nfrom transformers import DataCollatorWithPadding\nfrom transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer\nimport nlpaug","metadata":{"execution":{"iopub.status.busy":"2023-08-07T15:18:03.184999Z","iopub.execute_input":"2023-08-07T15:18:03.186064Z","iopub.status.idle":"2023-08-07T15:18:32.616626Z","shell.execute_reply.started":"2023-08-07T15:18:03.186014Z","shell.execute_reply":"2023-08-07T15:18:32.615554Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/__init__.py:98: UserWarning: unable to load libtensorflow_io_plugins.so: unable to open file: libtensorflow_io_plugins.so, from paths: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so']\ncaused by: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so: undefined symbol: _ZN3tsl6StatusC1EN10tensorflow5error4CodeESt17basic_string_viewIcSt11char_traitsIcEENS_14SourceLocationE']\n  warnings.warn(f\"unable to load libtensorflow_io_plugins.so: {e}\")\n/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/__init__.py:104: UserWarning: file system plugins are not loaded: unable to open file: libtensorflow_io.so, from paths: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io.so']\ncaused by: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io.so: undefined symbol: _ZTVN10tensorflow13GcsFileSystemE']\n  warnings.warn(f\"file system plugins are not loaded: {e}\")\n","output_type":"stream"}]},{"cell_type":"code","source":"nlp = spacy.load('en_core_web_sm')","metadata":{"execution":{"iopub.status.busy":"2023-08-07T15:18:32.617949Z","iopub.execute_input":"2023-08-07T15:18:32.618870Z","iopub.status.idle":"2023-08-07T15:18:34.381320Z","shell.execute_reply.started":"2023-08-07T15:18:32.618839Z","shell.execute_reply":"2023-08-07T15:18:34.379987Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"train_df = pd.read_csv(\"/kaggle/input/nlp-getting-started/train.csv\")\ntest_df = pd.read_csv(\"/kaggle/input/nlp-getting-started/test.csv\")","metadata":{"execution":{"iopub.status.busy":"2023-08-07T15:18:34.384647Z","iopub.execute_input":"2023-08-07T15:18:34.385302Z","iopub.status.idle":"2023-08-07T15:18:34.482977Z","shell.execute_reply.started":"2023-08-07T15:18:34.385264Z","shell.execute_reply":"2023-08-07T15:18:34.481676Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":"## Preproc","metadata":{}},{"cell_type":"code","source":"# def lemmatize_remove_stop(doc):\n#     return (\" \".join([token.lemma_ for token in nlp(doc) if not token.is_stop])).strip().lower()","metadata":{"execution":{"iopub.status.busy":"2023-08-07T15:18:34.484713Z","iopub.execute_input":"2023-08-07T15:18:34.485132Z","iopub.status.idle":"2023-08-07T15:18:34.490053Z","shell.execute_reply.started":"2023-08-07T15:18:34.485060Z","shell.execute_reply":"2023-08-07T15:18:34.489010Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"# train_df['text_preproc'] = train_df['text'].apply(lemmatize_remove_stop)\n# test_df['text_preproc'] = test_df['text'].apply(lemmatize_remove_stop)","metadata":{"execution":{"iopub.status.busy":"2023-08-07T15:18:34.491527Z","iopub.execute_input":"2023-08-07T15:18:34.492189Z","iopub.status.idle":"2023-08-07T15:18:34.501193Z","shell.execute_reply.started":"2023-08-07T15:18:34.492153Z","shell.execute_reply":"2023-08-07T15:18:34.500141Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"# X = train_df.set_index('id')\n# y = train_df.set_index('id')['target']\n\n# X_test = test_df.set_index('id')","metadata":{"execution":{"iopub.status.busy":"2023-08-07T15:18:34.502683Z","iopub.execute_input":"2023-08-07T15:18:34.503067Z","iopub.status.idle":"2023-08-07T15:18:34.511440Z","shell.execute_reply.started":"2023-08-07T15:18:34.503033Z","shell.execute_reply":"2023-08-07T15:18:34.510144Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"# count_vectorizer = feature_extraction.text.CountVectorizer(max_features=5000)\n# X_bow_raw = count_vectorizer.fit_transform(X['text'])\n# X_test_bow_raw = count_vectorizer.transform(X_test['text'])\n\n# count_vectorizer2 = feature_extraction.text.CountVectorizer(max_features=5000)\n# X_bow = count_vectorizer2.fit_transform(X['text_preproc']);\n# X_test_bow = count_vectorizer2.transform(X_test['text_preproc']);","metadata":{"execution":{"iopub.status.busy":"2023-08-07T15:18:34.514960Z","iopub.execute_input":"2023-08-07T15:18:34.515881Z","iopub.status.idle":"2023-08-07T15:18:34.528524Z","shell.execute_reply.started":"2023-08-07T15:18:34.515835Z","shell.execute_reply":"2023-08-07T15:18:34.527325Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"# tfidf_vectorizer = feature_extraction.text.TfidfVectorizer(max_features=5000)\n# X_tfidf_raw = tfidf_vectorizer.fit_transform(X['text'])\n# X_test_tfidf_raw = tfidf_vectorizer.transform(X_test['text'])\n\n# tfidf_vectorizer2 = feature_extraction.text.TfidfVectorizer(max_features=5000)\n# X_tfidf = tfidf_vectorizer2.fit_transform(X['text_preproc']);\n# X_test_tfidf = tfidf_vectorizer2.transform(X_test['text_preproc']);","metadata":{"execution":{"iopub.status.busy":"2023-08-07T15:18:34.530173Z","iopub.execute_input":"2023-08-07T15:18:34.530564Z","iopub.status.idle":"2023-08-07T15:18:34.538447Z","shell.execute_reply.started":"2023-08-07T15:18:34.530533Z","shell.execute_reply":"2023-08-07T15:18:34.537238Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"# X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.33, random_state=42)","metadata":{"execution":{"iopub.status.busy":"2023-08-07T15:18:34.542273Z","iopub.execute_input":"2023-08-07T15:18:34.542658Z","iopub.status.idle":"2023-08-07T15:18:34.557450Z","shell.execute_reply.started":"2023-08-07T15:18:34.542632Z","shell.execute_reply":"2023-08-07T15:18:34.556107Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"# # Bag of words\n\n# count_vectorizer = feature_extraction.text.CountVectorizer(max_features=5000)\n# X_bow = count_vectorizer.fit_transform(X['text']);\n\n# X_train_bow = count_vectorizer.transform(X_train['text'])\n# X_test_bow = count_vectorizer.transform(X_test['text'])\n\n# # TF-IDF\n\n# tf_idf_trans = feature_extraction.text.TfidfVectorizer(max_features=5000)\n# X_tf_idf = tf_idf_trans.fit_transform(X['text'])\n\n# X_train_tf_idf = tf_idf_trans.transform(X_train)\n# X_test_tf_idf = tf_idf_trans.transform(X_test)","metadata":{"execution":{"iopub.status.busy":"2023-08-07T15:18:34.558615Z","iopub.execute_input":"2023-08-07T15:18:34.558901Z","iopub.status.idle":"2023-08-07T15:18:34.577488Z","shell.execute_reply.started":"2023-08-07T15:18:34.558877Z","shell.execute_reply":"2023-08-07T15:18:34.576226Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"markdown","source":"## Modelling","metadata":{}},{"cell_type":"code","source":"# def cross_validate_helper(model, X, y):\n#     return pd.DataFrame(cross_validate(model, X, y, scoring=[\"f1\", \"accuracy\", \"r2\"], return_train_score=True)).agg(['mean', 'std'])","metadata":{"execution":{"iopub.status.busy":"2023-08-07T15:18:34.579137Z","iopub.execute_input":"2023-08-07T15:18:34.579701Z","iopub.status.idle":"2023-08-07T15:18:34.592354Z","shell.execute_reply.started":"2023-08-07T15:18:34.579669Z","shell.execute_reply":"2023-08-07T15:18:34.591219Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"# experiments = {\"naivebayes_tfidf_preproc\":dict(model=MultinomialNB(), X=X_tfidf, y=y),\n#                \"naivebayes_bow_preproc\":dict(model=MultinomialNB(), X=X_bow, y=y),\n#                 \"naivebayes_tfidf_raw\":dict(model=MultinomialNB(), X=X_tfidf_raw, y=y),\n#                 \"naivebayes_bow_raw\":dict(model=MultinomialNB(), X=X_bow_raw, y=y),    \n# }","metadata":{"execution":{"iopub.status.busy":"2023-08-07T15:18:34.593654Z","iopub.execute_input":"2023-08-07T15:18:34.594925Z","iopub.status.idle":"2023-08-07T15:18:34.602623Z","shell.execute_reply.started":"2023-08-07T15:18:34.594892Z","shell.execute_reply":"2023-08-07T15:18:34.600088Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"# pd.DataFrame([cross_validate_helper(**experiment_args).unstack().rename(experiment_name) \n#               for experiment_name, experiment_args in experiments.items()]\n#             ).sort_values(('test_f1','mean'),ascending=False)","metadata":{"execution":{"iopub.status.busy":"2023-08-07T15:18:34.604457Z","iopub.execute_input":"2023-08-07T15:18:34.606137Z","iopub.status.idle":"2023-08-07T15:18:34.611127Z","shell.execute_reply.started":"2023-08-07T15:18:34.606101Z","shell.execute_reply":"2023-08-07T15:18:34.609834Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"# def fit_return_metrics(X_train_input, y_train_input, X_test_input,y_test_input, model):\n#     \"\"\"Fit the model, and return train and test metrics\"\"\"\n\n#     model.fit(X_train_input, y_train_input);\n#     y_train_pred = model.predict(X_train_input)\n#     y_test_pred = model.predict(X_test_input)\n\n#     # train\n    \n#     metric_set = dict(f1=f1_score, \n#                       r2=r2_score,\n#                       accuracy = accuracy_score)\n    \n#     def metrics_dict(y_act, y_pred):\n#         return{metric : metric_func(y_act, y_pred) for metric, metric_func in metric_set.items()}\n      \n#     metrics_list = [dict(regime=\"train\", **metrics_dict(y_train_input,y_train_pred)),\n#                    dict(regime=\"test\", **metrics_dict(y_test_input, y_test_pred))\n#                    ]\n    \n#     return pd.DataFrame(metrics_list).set_index(\"regime\").unstack()","metadata":{"execution":{"iopub.status.busy":"2023-08-07T15:18:34.612967Z","iopub.execute_input":"2023-08-07T15:18:34.613905Z","iopub.status.idle":"2023-08-07T15:18:34.625109Z","shell.execute_reply.started":"2023-08-07T15:18:34.613774Z","shell.execute_reply":"2023-08-07T15:18:34.624000Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"# pd.DataFrame([fit_return_metrics(X_train_bow, y_train, X_test_bow,y_test,xgboost.XGBClassifier()).rename('xgboost_bow'),\n#               fit_return_metrics(X_train_tf_idf, y_train, X_test_tf_idf,y_test,xgboost.XGBClassifier()).rename('xgboost_tfidf'),\n# #               fit_return_metrics(X_train_bow.toarray(), y_train, X_test_bow.toarray(),y_test,GaussianProcessClassifier()).rename('gaussianprocess_bow'),\n# #               fit_return_metrics(X_train_bow.toarray(), y_train, X_test_tf_idf.toarray(),y_test,GaussianProcessClassifier()).rename('gaussianprocess_tfidf'),\n# #               fit_return_metrics(X_train_bow.toarray(), y_train, X_test_bow.toarray(),y_test,MultinomialNB()).rename('naivebayes_bow'),\n# #               fit_return_metrics(X_train_bow.toarray(), y_train, X_test_tf_idf.toarray(),y_test,MultinomialNB()).rename('naivebayes_tfidf')\n#              ])\n \n ","metadata":{"execution":{"iopub.status.busy":"2023-08-07T15:18:34.627046Z","iopub.execute_input":"2023-08-07T15:18:34.628283Z","iopub.status.idle":"2023-08-07T15:18:34.642849Z","shell.execute_reply.started":"2023-08-07T15:18:34.628246Z","shell.execute_reply":"2023-08-07T15:18:34.640297Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"# see this notebook for BERT implementation \n# https://www.kaggle.com/code/alexia/kerasnlp-starter-notebook-disaster-tweets","metadata":{"execution":{"iopub.status.busy":"2023-08-07T15:18:34.644928Z","iopub.execute_input":"2023-08-07T15:18:34.645870Z","iopub.status.idle":"2023-08-07T15:18:34.654253Z","shell.execute_reply.started":"2023-08-07T15:18:34.645831Z","shell.execute_reply":"2023-08-07T15:18:34.652891Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"markdown","source":"# nlpaug","metadata":{}},{"cell_type":"markdown","source":"NLPAUG can be used to augment the dataset, e.g. by backtranslating","metadata":{}},{"cell_type":"code","source":"import nlpaug.augmenter.word as nlpaugsent\n\ntext = 'The quick brown fox jumped over the lazy dog'\nback_translation_aug = nlpaugsent.BackTranslationAug(\n    from_model_name='facebook/wmt19-en-de', \n    to_model_name='facebook/wmt19-de-en'\n)\nback_translation_aug.augment(text)","metadata":{"execution":{"iopub.status.busy":"2023-08-07T15:24:29.993946Z","iopub.execute_input":"2023-08-07T15:24:29.994980Z","iopub.status.idle":"2023-08-07T15:25:01.316539Z","shell.execute_reply.started":"2023-08-07T15:24:29.994941Z","shell.execute_reply":"2023-08-07T15:25:01.315287Z"},"trusted":true},"execution_count":20,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading (…)lve/main/config.json:   0%|          | 0.00/825 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0a89916dbe734b3a9c3ec22f8470fddd"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading pytorch_model.bin:   0%|          | 0.00/1.08G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4c3216494b984abca7ad87b50ad9f3d6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)neration_config.json:   0%|          | 0.00/235 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bb531bdb6f69462aae5961ce688e465c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)lve/main/config.json:   0%|          | 0.00/825 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bd1bffdf3c8a476eac4866062f887a51"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading pytorch_model.bin:   0%|          | 0.00/1.08G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1b844e838a1c4b158a8e331110e7b549"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)neration_config.json:   0%|          | 0.00/260 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8195ac7e7faf4649b2eccfc30f7a732f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)okenizer_config.json:   0%|          | 0.00/67.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3a80251a9b3f461a93e8573d0ed2313d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)/main/vocab-src.json:   0%|          | 0.00/849k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"dc5a38b2c5f84e47a59367b994eff3dd"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)olve/main/merges.txt:   0%|          | 0.00/315k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"68abc182163e4b95889225ab9bc4bd35"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)okenizer_config.json:   0%|          | 0.00/67.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4d5fbf2aa2184a7f9bdba37902b69802"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)/main/vocab-src.json:   0%|          | 0.00/849k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0d9293252c8f476d8d8cde17bed4d21c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)olve/main/merges.txt:   0%|          | 0.00/315k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0a451d1f67604faebeaf184f68089215"}},"metadata":{}},{"execution_count":20,"output_type":"execute_result","data":{"text/plain":"['The speedy brown fox leapt over the lazy dog']"},"metadata":{}}]},{"cell_type":"code","source":"train_df[train_df['target']==1]['text'].apply(back_translation_aug.augment)","metadata":{"execution":{"iopub.status.busy":"2023-08-07T15:28:53.442667Z","iopub.execute_input":"2023-08-07T15:28:53.443102Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Bert","metadata":{}},{"cell_type":"markdown","source":"See these: \n- https://www.kaggle.com/code/pritishmishra/text-classification-with-distilbert-92-accuracy\n- https://huggingface.co/docs/transformers/tasks/sequence_classification\n\nGoing with Pytorch as the API seems way easier","metadata":{}},{"cell_type":"code","source":"model_name= \"roberta-base\" #\"distilbert-base-uncased\"","metadata":{"execution":{"iopub.status.busy":"2023-08-07T14:59:18.052884Z","iopub.execute_input":"2023-08-07T14:59:18.053252Z","iopub.status.idle":"2023-08-07T14:59:18.058159Z","shell.execute_reply.started":"2023-08-07T14:59:18.053223Z","shell.execute_reply":"2023-08-07T14:59:18.057219Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"tokenizer = AutoTokenizer.from_pretrained(model_name)\n\ndef preprocess_function(examples):\n    return tokenizer(examples[\"text\"], truncation=True)","metadata":{"execution":{"iopub.status.busy":"2023-08-07T14:59:18.213864Z","iopub.execute_input":"2023-08-07T14:59:18.214231Z","iopub.status.idle":"2023-08-07T14:59:19.032771Z","shell.execute_reply.started":"2023-08-07T14:59:18.214201Z","shell.execute_reply":"2023-08-07T14:59:19.031677Z"},"trusted":true},"execution_count":20,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading (…)lve/main/config.json:   0%|          | 0.00/481 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5b30e8301568486db903bd9720b1181d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)olve/main/vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"13ae162ec7174f05b8da43564cb51d6b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)olve/main/merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9026689ff359433796e7894e5c8635d6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)/main/tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"573450057e444e248fe290c15dd964ef"}},"metadata":{}}]},{"cell_type":"code","source":"train_dataset = datasets.Dataset.from_pandas(train_df[['text', 'target']].rename(columns={'target':'label'})).train_test_split(test_size=0.2)\ntrain_dataset_tokenized = train_dataset.map(preprocess_function, batched=True)\n\ntest_dataset = datasets.Dataset.from_pandas(test_df[['text']])#.train_test_split(test_size=0.2)\ntest_dataset_tokenized = test_dataset.map(preprocess_function, batched=True)","metadata":{"execution":{"iopub.status.busy":"2023-08-07T14:59:19.851300Z","iopub.execute_input":"2023-08-07T14:59:19.851695Z","iopub.status.idle":"2023-08-07T14:59:21.941998Z","shell.execute_reply.started":"2023-08-07T14:59:19.851664Z","shell.execute_reply":"2023-08-07T14:59:21.940879Z"},"trusted":true},"execution_count":21,"outputs":[{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/7 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ee2c1928abd64cc68ddc9c82b758b1a1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/2 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b34e3d6bd9b742ee89d435ab267473ad"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/4 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6651a4be99654e3f822759e096357823"}},"metadata":{}}]},{"cell_type":"code","source":"back_translation_aug.augment(test_dataset['text'])","metadata":{"execution":{"iopub.status.busy":"2023-08-07T15:14:05.917820Z","iopub.execute_input":"2023-08-07T15:14:05.918224Z","iopub.status.idle":"2023-08-07T15:15:39.356752Z","shell.execute_reply.started":"2023-08-07T15:14:05.918188Z","shell.execute_reply":"2023-08-07T15:15:39.355114Z"},"trusted":true},"execution_count":44,"outputs":[{"name":"stdout","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"\u001b[31m╭─\u001b[0m\u001b[31m──────────────────────────────\u001b[0m\u001b[31m \u001b[0m\u001b[1;31mTraceback \u001b[0m\u001b[1;2;31m(most recent call last)\u001b[0m\u001b[31m \u001b[0m\u001b[31m───────────────────────────────\u001b[0m\u001b[31m─╮\u001b[0m\n\u001b[31m│\u001b[0m in \u001b[92m<module>\u001b[0m:\u001b[94m1\u001b[0m                                                                                    \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m1 back_translation_aug.augment(test_dataset[\u001b[33m'\u001b[0m\u001b[33mtext\u001b[0m\u001b[33m'\u001b[0m])                                           \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m2 \u001b[0m                                                                                             \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m \u001b[2;33m/opt/conda/lib/python3.10/site-packages/nlpaug/\u001b[0m\u001b[1;33mbase_augmenter.py\u001b[0m:\u001b[94m98\u001b[0m in \u001b[92maugment\u001b[0m                   \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m 95 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[2m# PyTorch's augmenter\u001b[0m                                                          \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m 96 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[94melif\u001b[0m \u001b[96mself\u001b[0m.\u001b[91m__class__\u001b[0m.\u001b[91m__name__\u001b[0m \u001b[95min\u001b[0m [\u001b[33m'\u001b[0m\u001b[33mAbstSummAug\u001b[0m\u001b[33m'\u001b[0m, \u001b[33m'\u001b[0m\u001b[33mBackTranslationAug\u001b[0m\u001b[33m'\u001b[0m, \u001b[33m'\u001b[0m\u001b[33mConte\u001b[0m   \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m 97 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m\u001b[94mfor\u001b[0m _ \u001b[95min\u001b[0m \u001b[96mrange\u001b[0m(aug_num):                                                   \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m 98 \u001b[2m│   │   │   │   │   \u001b[0mresult = action_fx(clean_data)                                         \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m 99 \u001b[0m\u001b[2m│   │   │   │   │   \u001b[0m\u001b[94mif\u001b[0m \u001b[96misinstance\u001b[0m(result, \u001b[96mlist\u001b[0m):                                           \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m100 \u001b[0m\u001b[2m│   │   │   │   │   │   \u001b[0maugmented_results.extend(result)                                   \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m101 \u001b[0m\u001b[2m│   │   │   │   │   \u001b[0m\u001b[94melse\u001b[0m:                                                                  \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m \u001b[2;33m/opt/conda/lib/python3.10/site-packages/nlpaug/augmenter/word/\u001b[0m\u001b[1;33mback_translation.py\u001b[0m:\u001b[94m70\u001b[0m in          \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m \u001b[92msubstitute\u001b[0m                                                                                       \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m67 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mif\u001b[0m \u001b[95mnot\u001b[0m data:                                                                        \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m68 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[94mreturn\u001b[0m data                                                                     \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m69 \u001b[0m\u001b[2m│   │   \u001b[0m                                                                                    \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m70 \u001b[2m│   │   \u001b[0maugmented_text = \u001b[96mself\u001b[0m.model.predict(data)                                           \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m71 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mreturn\u001b[0m augmented_text                                                               \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m72 \u001b[0m\u001b[2m│   \u001b[0m                                                                                        \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m73 \u001b[0m\u001b[2m│   \u001b[0m\u001b[1;95m@classmethod\u001b[0m                                                                            \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m \u001b[2;33m/opt/conda/lib/python3.10/site-packages/nlpaug/model/lang_models/\u001b[0m\u001b[1;33mmachine_translation_transformer\u001b[0m \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m \u001b[1;33ms.py\u001b[0m:\u001b[94m39\u001b[0m in \u001b[92mpredict\u001b[0m                                                                               \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m36 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mreturn\u001b[0m \u001b[96mstr\u001b[0m(\u001b[96mself\u001b[0m.src_model.device)                                                   \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m37 \u001b[0m\u001b[2m│   \u001b[0m                                                                                        \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m38 \u001b[0m\u001b[2m│   \u001b[0m\u001b[94mdef\u001b[0m \u001b[92mpredict\u001b[0m(\u001b[96mself\u001b[0m, texts, target_words=\u001b[94mNone\u001b[0m, n=\u001b[94m1\u001b[0m):                                       \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m39 \u001b[2m│   │   \u001b[0msrc_translated_texts = \u001b[96mself\u001b[0m.translate_one_step_batched(texts, \u001b[96mself\u001b[0m.src_tokenizer    \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m40 \u001b[0m\u001b[2m│   │   \u001b[0mtgt_translated_texts = \u001b[96mself\u001b[0m.translate_one_step_batched(src_translated_texts, \u001b[96msel\u001b[0m    \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m41 \u001b[0m\u001b[2m│   │   \u001b[0m                                                                                    \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m42 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mreturn\u001b[0m tgt_translated_texts                                                         \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m \u001b[2;33m/opt/conda/lib/python3.10/site-packages/nlpaug/model/lang_models/\u001b[0m\u001b[1;33mmachine_translation_transformer\u001b[0m \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m \u001b[1;33ms.py\u001b[0m:\u001b[94m62\u001b[0m in \u001b[92mtranslate_one_step_batched\u001b[0m                                                            \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m59 \u001b[0m\u001b[2m│   │   │   │   \u001b[0mbatch = \u001b[96mtuple\u001b[0m(t.to(\u001b[96mself\u001b[0m.device) \u001b[94mfor\u001b[0m t \u001b[95min\u001b[0m batch)                             \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m60 \u001b[0m\u001b[2m│   │   │   │   \u001b[0minput_ids, attention_mask = batch                                           \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m61 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m                                                                            \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m62 \u001b[2m│   │   │   │   \u001b[0mtranslated_ids_batch = model.generate(                                      \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m63 \u001b[0m\u001b[2m│   │   │   │   │   \u001b[0minput_ids=input_ids, attention_mask=attention_mask,                     \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m64 \u001b[0m\u001b[2m│   │   │   │   │   \u001b[0mmax_length=\u001b[96mself\u001b[0m.max_length                                              \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m65 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m)                                                                           \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m \u001b[2;33m/opt/conda/lib/python3.10/site-packages/torch/utils/\u001b[0m\u001b[1;33m_contextlib.py\u001b[0m:\u001b[94m115\u001b[0m in \u001b[92mdecorate_context\u001b[0m       \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m112 \u001b[0m\u001b[2m│   \u001b[0m\u001b[1;95m@functools\u001b[0m.wraps(func)                                                                 \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m113 \u001b[0m\u001b[2m│   \u001b[0m\u001b[94mdef\u001b[0m \u001b[92mdecorate_context\u001b[0m(*args, **kwargs):                                                 \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m114 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mwith\u001b[0m ctx_factory():                                                                \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m115 \u001b[2m│   │   │   \u001b[0m\u001b[94mreturn\u001b[0m func(*args, **kwargs)                                                   \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m116 \u001b[0m\u001b[2m│   \u001b[0m                                                                                       \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m117 \u001b[0m\u001b[2m│   \u001b[0m\u001b[94mreturn\u001b[0m decorate_context                                                                \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m118 \u001b[0m                                                                                           \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m \u001b[2;33m/opt/conda/lib/python3.10/site-packages/transformers/generation/\u001b[0m\u001b[1;33mutils.py\u001b[0m:\u001b[94m1611\u001b[0m in \u001b[92mgenerate\u001b[0m        \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m1608 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m**model_kwargs,                                                           \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m1609 \u001b[0m\u001b[2m│   │   │   \u001b[0m)                                                                             \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m1610 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[2m# 13. run beam search\u001b[0m                                                         \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m1611 \u001b[2m│   │   │   \u001b[0m\u001b[94mreturn\u001b[0m \u001b[96mself\u001b[0m.beam_search(                                                      \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m1612 \u001b[0m\u001b[2m│   │   │   │   \u001b[0minput_ids,                                                                \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m1613 \u001b[0m\u001b[2m│   │   │   │   \u001b[0mbeam_scorer,                                                              \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m1614 \u001b[0m\u001b[2m│   │   │   │   \u001b[0mlogits_processor=logits_processor,                                        \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m \u001b[2;33m/opt/conda/lib/python3.10/site-packages/transformers/generation/\u001b[0m\u001b[1;33mutils.py\u001b[0m:\u001b[94m2982\u001b[0m in \u001b[92mbeam_search\u001b[0m     \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m2979 \u001b[0m\u001b[2m│   │   │   │   \u001b[0moutputs, model_kwargs, is_encoder_decoder=\u001b[96mself\u001b[0m.config.is_encoder_decoder  \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m2980 \u001b[0m\u001b[2m│   │   │   \u001b[0m)                                                                             \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m2981 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[94mif\u001b[0m model_kwargs[\u001b[33m\"\u001b[0m\u001b[33mpast_key_values\u001b[0m\u001b[33m\"\u001b[0m] \u001b[95mis\u001b[0m \u001b[95mnot\u001b[0m \u001b[94mNone\u001b[0m:                               \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m2982 \u001b[2m│   │   │   │   \u001b[0mmodel_kwargs[\u001b[33m\"\u001b[0m\u001b[33mpast_key_values\u001b[0m\u001b[33m\"\u001b[0m] = \u001b[96mself\u001b[0m._reorder_cache(model_kwargs[\u001b[33m\"\u001b[0m\u001b[33mpast\u001b[0m  \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m2983 \u001b[0m\u001b[2m│   │   │   \u001b[0m                                                                              \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m2984 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[94mif\u001b[0m return_dict_in_generate \u001b[95mand\u001b[0m output_scores:                                 \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m2985 \u001b[0m\u001b[2m│   │   │   │   \u001b[0mbeam_indices = \u001b[96mtuple\u001b[0m((beam_indices[beam_idx[i]] + (beam_idx[i],) \u001b[94mfor\u001b[0m i \u001b[95mi\u001b[0m  \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m \u001b[2;33m/opt/conda/lib/python3.10/site-packages/transformers/models/fsmt/\u001b[0m\u001b[1;33mmodeling_fsmt.py\u001b[0m:\u001b[94m1301\u001b[0m in        \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m \u001b[92m_reorder_cache\u001b[0m                                                                                   \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m1298 \u001b[0m\u001b[2m│   │   \u001b[0mreordered_past = []                                                               \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m1299 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mfor\u001b[0m layer_past \u001b[95min\u001b[0m past_key_values:                                                \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m1300 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[2m# get the correct batch idx from decoder layer's batch dim for cross and sel\u001b[0m  \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m1301 \u001b[2m│   │   │   \u001b[0mlayer_past_new = {                                                            \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m1302 \u001b[0m\u001b[2m│   │   │   │   \u001b[0mattn_key: _reorder_buffer(attn_cache, beam_idx) \u001b[94mfor\u001b[0m attn_key, attn_cache  \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m1303 \u001b[0m\u001b[2m│   │   │   \u001b[0m}                                                                             \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m1304 \u001b[0m\u001b[2m│   │   │   \u001b[0mreordered_past.append(layer_past_new)                                         \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m \u001b[2;33m/opt/conda/lib/python3.10/site-packages/transformers/models/fsmt/\u001b[0m\u001b[1;33mmodeling_fsmt.py\u001b[0m:\u001b[94m1302\u001b[0m in        \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m \u001b[92m<dictcomp>\u001b[0m                                                                                       \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m1299 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mfor\u001b[0m layer_past \u001b[95min\u001b[0m past_key_values:                                                \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m1300 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[2m# get the correct batch idx from decoder layer's batch dim for cross and sel\u001b[0m  \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m1301 \u001b[0m\u001b[2m│   │   │   \u001b[0mlayer_past_new = {                                                            \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m1302 \u001b[2m│   │   │   │   \u001b[0mattn_key: _reorder_buffer(attn_cache, beam_idx) \u001b[94mfor\u001b[0m attn_key, attn_cache  \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m1303 \u001b[0m\u001b[2m│   │   │   \u001b[0m}                                                                             \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m1304 \u001b[0m\u001b[2m│   │   │   \u001b[0mreordered_past.append(layer_past_new)                                         \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m1305 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mreturn\u001b[0m reordered_past                                                             \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m \u001b[2;33m/opt/conda/lib/python3.10/site-packages/transformers/models/fsmt/\u001b[0m\u001b[1;33mmodeling_fsmt.py\u001b[0m:\u001b[94m852\u001b[0m in         \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m \u001b[92m_reorder_buffer\u001b[0m                                                                                  \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m 849 \u001b[0m\u001b[94mdef\u001b[0m \u001b[92m_reorder_buffer\u001b[0m(attn_cache, new_order):                                               \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m 850 \u001b[0m\u001b[2m│   \u001b[0m\u001b[94mfor\u001b[0m k, input_buffer_k \u001b[95min\u001b[0m attn_cache.items():                                          \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m 851 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mif\u001b[0m input_buffer_k \u001b[95mis\u001b[0m \u001b[95mnot\u001b[0m \u001b[94mNone\u001b[0m:                                                    \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m 852 \u001b[2m│   │   │   \u001b[0mattn_cache[k] = input_buffer_k.index_select(\u001b[94m0\u001b[0m, new_order)                     \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m 853 \u001b[0m\u001b[2m│   \u001b[0m\u001b[94mreturn\u001b[0m attn_cache                                                                     \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m 854 \u001b[0m                                                                                          \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m 855 \u001b[0m                                                                                          \u001b[31m│\u001b[0m\n\u001b[31m╰──────────────────────────────────────────────────────────────────────────────────────────────────╯\u001b[0m\n\u001b[1;91mKeyboardInterrupt\u001b[0m\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800000; text-decoration-color: #800000\">╭─────────────────────────────── </span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">Traceback </span><span style=\"color: #bf7f7f; text-decoration-color: #bf7f7f; font-weight: bold\">(most recent call last)</span><span style=\"color: #800000; text-decoration-color: #800000\"> ────────────────────────────────╮</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">&lt;module&gt;</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">1</span>                                                                                    <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>1 back_translation_aug.augment(test_dataset[<span style=\"color: #808000; text-decoration-color: #808000\">'text'</span>])                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">2 </span>                                                                                             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/opt/conda/lib/python3.10/site-packages/nlpaug/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">base_augmenter.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">98</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">augment</span>                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 95 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"># PyTorch's augmenter</span>                                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 96 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">elif</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.<span style=\"color: #ff0000; text-decoration-color: #ff0000\">__class__</span>.<span style=\"color: #ff0000; text-decoration-color: #ff0000\">__name__</span> <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">in</span> [<span style=\"color: #808000; text-decoration-color: #808000\">'AbstSummAug'</span>, <span style=\"color: #808000; text-decoration-color: #808000\">'BackTranslationAug'</span>, <span style=\"color: #808000; text-decoration-color: #808000\">'Conte</span>   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 97 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">for</span> _ <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">in</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">range</span>(aug_num):                                                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span> 98 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   │   </span>result = action_fx(clean_data)                                         <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 99 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">isinstance</span>(result, <span style=\"color: #00ffff; text-decoration-color: #00ffff\">list</span>):                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">100 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   │   │   </span>augmented_results.extend(result)                                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">101 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">else</span>:                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/opt/conda/lib/python3.10/site-packages/nlpaug/augmenter/word/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">back_translation.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">70</span> in          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00\">substitute</span>                                                                                       <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">67 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">not</span> data:                                                                        <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">68 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> data                                                                     <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">69 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>                                                                                    <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>70 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>augmented_text = <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.model.predict(data)                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">71 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> augmented_text                                                               <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">72 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   </span>                                                                                        <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">73 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   </span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff; font-weight: bold\">@classmethod</span>                                                                            <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/opt/conda/lib/python3.10/site-packages/nlpaug/model/lang_models/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">machine_translation_transformer</span> <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">s.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">39</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">predict</span>                                                                               <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">36 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">str</span>(<span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.src_model.device)                                                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">37 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   </span>                                                                                        <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">38 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">def</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00\">predict</span>(<span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>, texts, target_words=<span style=\"color: #0000ff; text-decoration-color: #0000ff\">None</span>, n=<span style=\"color: #0000ff; text-decoration-color: #0000ff\">1</span>):                                       <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>39 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>src_translated_texts = <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.translate_one_step_batched(texts, <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.src_tokenizer    <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">40 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>tgt_translated_texts = <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.translate_one_step_batched(src_translated_texts, <span style=\"color: #00ffff; text-decoration-color: #00ffff\">sel</span>    <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">41 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>                                                                                    <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">42 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> tgt_translated_texts                                                         <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/opt/conda/lib/python3.10/site-packages/nlpaug/model/lang_models/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">machine_translation_transformer</span> <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">s.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">62</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">translate_one_step_batched</span>                                                            <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">59 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   </span>batch = <span style=\"color: #00ffff; text-decoration-color: #00ffff\">tuple</span>(t.to(<span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.device) <span style=\"color: #0000ff; text-decoration-color: #0000ff\">for</span> t <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">in</span> batch)                             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">60 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   </span>input_ids, attention_mask = batch                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">61 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   </span>                                                                            <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>62 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   </span>translated_ids_batch = model.generate(                                      <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">63 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   │   </span>input_ids=input_ids, attention_mask=attention_mask,                     <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">64 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   │   </span>max_length=<span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.max_length                                              <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">65 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   </span>)                                                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/opt/conda/lib/python3.10/site-packages/torch/utils/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">_contextlib.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">115</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">decorate_context</span>       <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">112 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   </span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff; font-weight: bold\">@functools</span>.wraps(func)                                                                 <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">113 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">def</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00\">decorate_context</span>(*args, **kwargs):                                                 <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">114 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">with</span> ctx_factory():                                                                <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>115 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> func(*args, **kwargs)                                                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">116 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   </span>                                                                                       <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">117 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> decorate_context                                                                <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">118 </span>                                                                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/opt/conda/lib/python3.10/site-packages/transformers/generation/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">utils.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">1611</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">generate</span>        <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1608 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   </span>**model_kwargs,                                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1609 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span>)                                                                             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1610 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"># 13. run beam search</span>                                                         <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>1611 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.beam_search(                                                      <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1612 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   </span>input_ids,                                                                <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1613 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   </span>beam_scorer,                                                              <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1614 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   </span>logits_processor=logits_processor,                                        <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/opt/conda/lib/python3.10/site-packages/transformers/generation/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">utils.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">2982</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">beam_search</span>     <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">2979 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   </span>outputs, model_kwargs, is_encoder_decoder=<span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.config.is_encoder_decoder  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">2980 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span>)                                                                             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">2981 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> model_kwargs[<span style=\"color: #808000; text-decoration-color: #808000\">\"past_key_values\"</span>] <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">is</span> <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">not</span> <span style=\"color: #0000ff; text-decoration-color: #0000ff\">None</span>:                               <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>2982 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   </span>model_kwargs[<span style=\"color: #808000; text-decoration-color: #808000\">\"past_key_values\"</span>] = <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._reorder_cache(model_kwargs[<span style=\"color: #808000; text-decoration-color: #808000\">\"past</span>  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">2983 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span>                                                                              <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">2984 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> return_dict_in_generate <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">and</span> output_scores:                                 <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">2985 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   </span>beam_indices = <span style=\"color: #00ffff; text-decoration-color: #00ffff\">tuple</span>((beam_indices[beam_idx[i]] + (beam_idx[i],) <span style=\"color: #0000ff; text-decoration-color: #0000ff\">for</span> i <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">i</span>  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/opt/conda/lib/python3.10/site-packages/transformers/models/fsmt/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">modeling_fsmt.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">1301</span> in        <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00\">_reorder_cache</span>                                                                                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1298 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>reordered_past = []                                                               <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1299 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">for</span> layer_past <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">in</span> past_key_values:                                                <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1300 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"># get the correct batch idx from decoder layer's batch dim for cross and sel</span>  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>1301 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span>layer_past_new = {                                                            <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1302 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   </span>attn_key: _reorder_buffer(attn_cache, beam_idx) <span style=\"color: #0000ff; text-decoration-color: #0000ff\">for</span> attn_key, attn_cache  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1303 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span>}                                                                             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1304 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span>reordered_past.append(layer_past_new)                                         <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/opt/conda/lib/python3.10/site-packages/transformers/models/fsmt/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">modeling_fsmt.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">1302</span> in        <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00\">&lt;dictcomp&gt;</span>                                                                                       <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1299 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">for</span> layer_past <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">in</span> past_key_values:                                                <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1300 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"># get the correct batch idx from decoder layer's batch dim for cross and sel</span>  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1301 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span>layer_past_new = {                                                            <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>1302 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   </span>attn_key: _reorder_buffer(attn_cache, beam_idx) <span style=\"color: #0000ff; text-decoration-color: #0000ff\">for</span> attn_key, attn_cache  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1303 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span>}                                                                             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1304 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span>reordered_past.append(layer_past_new)                                         <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1305 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> reordered_past                                                             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/opt/conda/lib/python3.10/site-packages/transformers/models/fsmt/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">modeling_fsmt.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">852</span> in         <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00\">_reorder_buffer</span>                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 849 </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">def</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00\">_reorder_buffer</span>(attn_cache, new_order):                                               <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 850 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">for</span> k, input_buffer_k <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">in</span> attn_cache.items():                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 851 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> input_buffer_k <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">is</span> <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">not</span> <span style=\"color: #0000ff; text-decoration-color: #0000ff\">None</span>:                                                    <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span> 852 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span>attn_cache[k] = input_buffer_k.index_select(<span style=\"color: #0000ff; text-decoration-color: #0000ff\">0</span>, new_order)                     <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 853 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> attn_cache                                                                     <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 854 </span>                                                                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 855 </span>                                                                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">╰──────────────────────────────────────────────────────────────────────────────────────────────────╯</span>\n<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-weight: bold\">KeyboardInterrupt</span>\n</pre>\n"},"metadata":{}}]},{"cell_type":"code","source":"data_collator = DataCollatorWithPadding(tokenizer=tokenizer)#, return_tensors=\"tf\")","metadata":{"execution":{"iopub.status.busy":"2023-08-07T14:59:21.944226Z","iopub.execute_input":"2023-08-07T14:59:21.944671Z","iopub.status.idle":"2023-08-07T14:59:21.951549Z","shell.execute_reply.started":"2023-08-07T14:59:21.944636Z","shell.execute_reply":"2023-08-07T14:59:21.950242Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"code","source":"model_params = dict(id2label = {0: 0, 1:1},\n                    label2id = {0: 0, 1: 1},\n                    num_labels=2)\n\nmodel = AutoModelForSequenceClassification.from_pretrained(model_name, **model_params)","metadata":{"execution":{"iopub.status.busy":"2023-08-07T14:59:21.953050Z","iopub.execute_input":"2023-08-07T14:59:21.953532Z","iopub.status.idle":"2023-08-07T14:59:26.918792Z","shell.execute_reply.started":"2023-08-07T14:59:21.953500Z","shell.execute_reply":"2023-08-07T14:59:26.917820Z"},"trusted":true},"execution_count":23,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading model.safetensors:   0%|          | 0.00/499M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a97c841a5ca44d99986198f40feb352a"}},"metadata":{}},{"name":"stderr","text":"Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.dense.bias', 'lm_head.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.layer_norm.bias']\n- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nSome weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.dense.bias', 'classifier.out_proj.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"}]},{"cell_type":"code","source":"def compute_metrics(pred):\n    labels = pred.label_ids\n    preds = pred.predictions.argmax(-1)\n    f1 = f1_score(labels, preds, average=\"weighted\")\n    acc = accuracy_score(labels, preds)\n    return {\"accuracy\": acc, \"f1\": f1}","metadata":{"execution":{"iopub.status.busy":"2023-08-07T14:59:26.921769Z","iopub.execute_input":"2023-08-07T14:59:26.922407Z","iopub.status.idle":"2023-08-07T14:59:26.929027Z","shell.execute_reply.started":"2023-08-07T14:59:26.922369Z","shell.execute_reply":"2023-08-07T14:59:26.927749Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"code","source":"batch_size = 64\nlogging_steps = len(train_dataset_tokenized[\"train\"]) // batch_size\n\ntraining_args = TrainingArguments(\n    output_dir=f\"{model_name}_finetuned_natural_disaster_tweets_model\",\n    learning_rate=2e-5,\n    per_device_train_batch_size=batch_size,\n    per_device_eval_batch_size=batch_size,\n    num_train_epochs=5,\n    weight_decay=0.01,\n    evaluation_strategy=\"epoch\",\n    save_strategy=\"epoch\",\n    load_best_model_at_end=True,\n    push_to_hub=False,\n    # added these\n    #metrics_for_best_model='f1',\n    #greater_is_better=True,\n    eval_steps=batch_size,\n    logging_steps=batch_size,\n    report_to=\"none\"\n)","metadata":{"execution":{"iopub.status.busy":"2023-08-07T14:59:26.930638Z","iopub.execute_input":"2023-08-07T14:59:26.931319Z","iopub.status.idle":"2023-08-07T14:59:26.946799Z","shell.execute_reply.started":"2023-08-07T14:59:26.931285Z","shell.execute_reply":"2023-08-07T14:59:26.945840Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"code","source":"trainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=train_dataset_tokenized[\"train\"],\n    eval_dataset=train_dataset_tokenized[\"test\"],\n    tokenizer=tokenizer,\n    data_collator=data_collator,\n    compute_metrics=compute_metrics,\n)","metadata":{"execution":{"iopub.status.busy":"2023-08-07T14:59:26.951070Z","iopub.execute_input":"2023-08-07T14:59:26.951688Z","iopub.status.idle":"2023-08-07T14:59:32.638531Z","shell.execute_reply.started":"2023-08-07T14:59:26.951650Z","shell.execute_reply":"2023-08-07T14:59:32.637491Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"code","source":"trainer.train()","metadata":{"execution":{"iopub.status.busy":"2023-08-07T14:59:32.640181Z","iopub.execute_input":"2023-08-07T14:59:32.641230Z","iopub.status.idle":"2023-08-07T15:03:25.315612Z","shell.execute_reply.started":"2023-08-07T14:59:32.641194Z","shell.execute_reply":"2023-08-07T15:03:25.314630Z"},"trusted":true},"execution_count":27,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  warnings.warn(\nYou're using a RobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='240' max='240' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [240/240 03:45, Epoch 5/5]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n      <th>F1</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>No log</td>\n      <td>0.416600</td>\n      <td>0.824032</td>\n      <td>0.824062</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.474700</td>\n      <td>0.393675</td>\n      <td>0.829941</td>\n      <td>0.827692</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.349100</td>\n      <td>0.399391</td>\n      <td>0.833224</td>\n      <td>0.832560</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>0.298700</td>\n      <td>0.418871</td>\n      <td>0.827971</td>\n      <td>0.827910</td>\n    </tr>\n    <tr>\n      <td>5</td>\n      <td>0.298700</td>\n      <td>0.421383</td>\n      <td>0.833880</td>\n      <td>0.833396</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n","output_type":"stream"},{"execution_count":27,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=240, training_loss=0.35364180405934653, metrics={'train_runtime': 232.2794, 'train_samples_per_second': 131.092, 'train_steps_per_second': 1.033, 'total_flos': 1052160554833440.0, 'train_loss': 0.35364180405934653, 'epoch': 5.0})"},"metadata":{}}]},{"cell_type":"code","source":"preds_output = trainer.predict(train_dataset_tokenized[\"test\"])\n\ny_preds = np.argmax(preds_output.predictions, axis=1)\n\npreds_output.metrics","metadata":{"execution":{"iopub.status.busy":"2023-08-07T15:03:25.317273Z","iopub.execute_input":"2023-08-07T15:03:25.317632Z","iopub.status.idle":"2023-08-07T15:03:28.778053Z","shell.execute_reply.started":"2023-08-07T15:03:25.317598Z","shell.execute_reply":"2023-08-07T15:03:28.777032Z"},"trusted":true},"execution_count":28,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"execution_count":28,"output_type":"execute_result","data":{"text/plain":"{'test_loss': 0.3936748206615448,\n 'test_accuracy': 0.8299409061063691,\n 'test_f1': 0.8276918241316522,\n 'test_runtime': 3.4483,\n 'test_samples_per_second': 441.663,\n 'test_steps_per_second': 3.48}"},"metadata":{}}]},{"cell_type":"markdown","source":"# Submit best model","metadata":{}},{"cell_type":"code","source":"# best_model = MultinomialNB()\n\n# best_model.fit(X_bow_raw, y)\n\n# y_pred_test  = best_model.predict(X_test_bow_raw)\n\npreds_output = trainer.predict(test_dataset_tokenized)\n\ny_pred_test = np.argmax(preds_output.predictions, axis=1)\n\npreds_output.metrics","metadata":{"execution":{"iopub.status.busy":"2023-08-07T15:03:28.780561Z","iopub.execute_input":"2023-08-07T15:03:28.781337Z","iopub.status.idle":"2023-08-07T15:03:36.127732Z","shell.execute_reply.started":"2023-08-07T15:03:28.781299Z","shell.execute_reply":"2023-08-07T15:03:36.126799Z"},"trusted":true},"execution_count":29,"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"execution_count":29,"output_type":"execute_result","data":{"text/plain":"{'test_runtime': 7.3314,\n 'test_samples_per_second': 445.07,\n 'test_steps_per_second': 3.546}"},"metadata":{}}]},{"cell_type":"code","source":"# Create a submission dataframe\nsubmission = pd.DataFrame({'id': test_df['id'], 'target': y_pred_test})\n\n# Write the submission dataframe to a CSV file\nsubmission.to_csv('submission.csv', index=False)\n\nsubmission.head()","metadata":{"execution":{"iopub.status.busy":"2023-08-07T15:03:36.133751Z","iopub.execute_input":"2023-08-07T15:03:36.136469Z","iopub.status.idle":"2023-08-07T15:03:36.172301Z","shell.execute_reply.started":"2023-08-07T15:03:36.136429Z","shell.execute_reply":"2023-08-07T15:03:36.171493Z"},"trusted":true},"execution_count":30,"outputs":[{"execution_count":30,"output_type":"execute_result","data":{"text/plain":"   id  target\n0   0       1\n1   2       1\n2   3       1\n3   9       1\n4  11       1","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>target</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>3</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>9</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>11</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}